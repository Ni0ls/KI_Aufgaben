{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c86ba8e",
   "metadata": {},
   "source": [
    "#  Monte Carlo Methods and Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906414c3",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "    A) What is the difference between Monte Carlo (MC) methods and Dynamic Programming?\n",
    "        -> MC lernt durch erfahrung ohne das genaue Model des Systems zu kennen, DP lernt durch das gegebene Model mit\n",
    "        bootsatraping\n",
    "    B) What is bootstrapping? Is MC using bootstrapping?\n",
    "        -> MC benutzt kein bootstraping, bootstraping bedeuted das bisherige wissen zu benutzen um neues zu generieren\n",
    "    C) What is the difference between on-policy and off-policy?\n",
    "        -> on/off policy beschreibt die auswahl der eintscheidungen von aktionen a im zustand s, bei on-policy wird die \n",
    "        target policy (optimale) evaluiert und verbessert, bei off-policy wird eine zur target policy andere policy \n",
    "        verwendet um neue pfade zu erkunden (diese muss sich mit der target policy überscheiden)\n",
    "    D) What is the following equation describing? (pt)\n",
    "        -> importance-sampling ratio: verhältnis zwischen der übereinstimmung von on zu off policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9f1c0",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f4edced",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22340/4218344268.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpygame\u001b[0m    \u001b[1;31m# used for delaying pygame visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Exercise for AIM course - Reinforcement Learning\n",
    "\n",
    "Example Code for\n",
    "- Monte Carlo Method\n",
    "- Q Learning \n",
    "\n",
    "Using Open AI Gym's Frozen Lake Environment    \n",
    "https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\n",
    "> pip install gym     # to install gym lib\n",
    "> pip install pygame  # to install pygame lib (needed for visualization) \n",
    "\n",
    "Suggested further reading:\n",
    "https://blog.paperspace.com/getting-started-with-openai-gym/\n",
    "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "\n",
    "@author: A. Hanuschkin 22\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import gym \n",
    "import pygame    # used for delaying pygame visualization \n",
    "import numpy as np\n",
    "from collections import deque \n",
    "\n",
    "# use matplotlib for plotting & animation \n",
    "# and increase default font size of figures \n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.animation as ma\n",
    "font_size = 16 \n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : font_size}\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q,obs,epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Implementation of epsilon greedy policy\n",
    "  \n",
    "    Parameters:\n",
    "    Q nxm array (float): Q Table of n states and m actions\n",
    "    obs         (int):  current state\n",
    "    epsilon     (float): epsilon used in epsilon greedy algorithm\n",
    "  \n",
    "    Returns:\n",
    "    int: action \n",
    "    \"\"\" \n",
    "    #***YOUR CODE HERE***\n",
    "    if epsilon < np.random.rand(): \n",
    "        return np.random.randint(0,4) #with probability of epsilon, return a random action = [0-3]\n",
    "    else:\n",
    "        return np.argmax(Q[obs][:])   #otherwise choose the greedy option (argmax of Q, best known option)\n",
    "\n",
    "\n",
    "\n",
    "def plot_training(x,y,fout='Q_Q-Learning.jpg'):    \n",
    "    \"\"\"\n",
    "    Plot of Training Progress Saved to File\n",
    "  \n",
    "    Parameters:\n",
    "    x array (float): x values\n",
    "    y array (float): y values\n",
    "    fout    (str): save plot to filename <fout>\n",
    "  \n",
    "    Returns:\n",
    "    \"\"\"     \n",
    "    plt.figure()\n",
    "    plt.plot(x,y,scores,'X', color='black')\n",
    "    plt.plot(x,y,scores,'--', color='black')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel(r'$<reward_{final}>$')\n",
    "    plt.savefig(fout,dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def animate(i,Q,ax):\n",
    "    \"\"\"\n",
    "    Animation function called by matplotlib FuncAnimation\n",
    "  \n",
    "    Parameters:\n",
    "    Q nxm array (float): Q Table of n states and m actions\n",
    "    ax <AxesSubplot:>: current axis to draw\n",
    "  \n",
    "    Returns:\n",
    "    \"\"\" \n",
    "    global obs\n",
    "    global walk_str\n",
    "    action = epsilon_greedy(Q,obs,epsilon = 0)\n",
    "    new_obs, reward, done, info = env.step(action)\n",
    "    obs = new_obs\n",
    "    print('state',obs,'action:',action_str[action], 'reward',reward)\n",
    "    walk_str = walk_str + '-' + action_str[action][0]\n",
    "    if done:\n",
    "        walk_str = walk_str + '-T'\n",
    "        if obs != 15:\n",
    "           print('Broken into the ice...')\n",
    "        else: \n",
    "           print('Done!!')\n",
    "        \n",
    "    env_screen = env.render(mode = 'rgb_array')\n",
    "    ax.clear()\n",
    "    plt.title(walk_str)\n",
    "    ax.imshow(env_screen)\n",
    "    plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        walk_str = 'S'\n",
    "\n",
    "def animation_result(Q):\n",
    "    \"\"\"\n",
    "    Animation of agent for given Q table\n",
    "  \n",
    "    Parameters:\n",
    "    Q nxm array (float): Q Table of n states and m actions\n",
    "\n",
    "    Returns:\n",
    "    animation object # store the created Animation in a variable that lives as long as the animation should run.\n",
    "    \"\"\" \n",
    "    global obs\n",
    "    global walk_str\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    obs = env.reset()\n",
    "    walk_str = 'S'\n",
    "\n",
    "    ani = ma.FuncAnimation(fig, animate, fargs = [Q,ax],frames=20, interval=1000, repeat=False)\n",
    "    #ani.save('test.mp4', fps=10)\n",
    "    #ani.save('test.gif',writer='imagemagick')\n",
    "\n",
    "    return ani \n",
    "    \n",
    "   # plt.show()\n",
    "   # pygame.time.wait(5000)\n",
    "\n",
    "\n",
    "#%% Setting up OPEN AI GYM environment & demo it's usage\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None,map_name=\"4x4\", is_slippery=False) ## is_slippery=True\n",
    "\n",
    "# observation and action space \n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))\n",
    "\n",
    "# reset the environment and see the initial observation\n",
    "obs = env.reset()\n",
    "print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "# sample a random action from the entire action space\n",
    "random_action = env.action_space.sample()\n",
    "print('Perform random action',random_action)\n",
    "\n",
    "# take the action and get the new observation space\n",
    "new_obs, reward, done, info = env.step(random_action)\n",
    "print(\"The new observation is {}\".format(new_obs))\n",
    "\n",
    "action_str = ['LEFT','DOWN','RIGHT','UP']\n",
    "\n",
    "'''\n",
    "for jj in range(10):  # visialize 10 random steps \n",
    "    env.render(mode = \"human\")\n",
    "    random_action = env.action_space.sample()\n",
    "    new_obs, reward, done, info = env.step(random_action)\n",
    "    print('action:',action_str[random_action])\n",
    "    # making the script wait for 50 ms\n",
    "    pygame.time.wait(50)\n",
    "'''  \n",
    "\n",
    "\n",
    "\n",
    "#%% Implement MC Method \n",
    "Q = np.zeros([16,4]) # init Q table \n",
    "Returns = np.zeros([16,4]) # init Q table \n",
    "N = np.zeros([16,4]) # init Q table \n",
    "pi = np.zeros(16) # init Q table \n",
    "\n",
    "# alternative way, not using a tabular but more felxible dictonary for Q\n",
    "#from collections import defaultdict\n",
    "#Q = defaultdict(lambda: np.zeros(4))\n",
    "\n",
    "nr_episodes = 20000\n",
    "for episode in range(nr_episodes):\n",
    "    # generate sequence of MC moves:     \n",
    "    # reset the environment and see the initial observation\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    seq = []\n",
    "    \n",
    "    while not(done):\n",
    "        #***YOUR CODE HERE***\n",
    "        # sample a epsilon-greedy action from the action space\n",
    "        action = epsilon_greedy(Q,obs)\n",
    "        # take the action and get the new observation space\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        seq.append((obs, action, reward))\n",
    "        obs = new_obs         \n",
    "        if done:\n",
    "            print(reward)\n",
    "    \n",
    "    ##print(seq)\n",
    "    # MC learning based on sampled seq \n",
    "    steps = len(seq) \n",
    "    G = 0 \n",
    "    gamma = 0.99\n",
    "    for jj in range(steps):\n",
    "        pos = steps-jj-1\n",
    "        G = gamma*G + seq[pos][2] \n",
    "        # check if first visit....\n",
    "        first_visit = True\n",
    "        for ii in range(steps-jj-1):  # check all states visited in this run \n",
    "            if seq[ii][0] == seq[pos][0] and seq[ii][1] == seq[pos][1]:\n",
    "                first_visit = False\n",
    "        if first_visit:    \n",
    "            #***YOUR CODE HERE***\n",
    "            s = [seq[pos][0]]\n",
    "            a = [seq[pos][1]]\n",
    "            Returns[s][a] = G #TODO what does append mean in algorithm ? question\n",
    "            Q[s][a] = Q[s][a] + 1/N[s][a]*(Returns[s][a]-Q[s][a])\n",
    "            pi[s] = np.argmax(Q[s][:])\n",
    "            \n",
    "#np.savez('Q_MC.npz',Q=Q) # uncomment to save your result\n",
    "\n",
    "\n",
    "#%% Q learning\n",
    "Q = np.random.rand(16,4)/10 # init Q table \n",
    "alpha = 0.05\n",
    "gamma = 0.99\n",
    "debug_ = False\n",
    "\n",
    "nr_episodes = 20000\n",
    "\n",
    "scores_window = deque(maxlen=1000)  # last <maxlen> scores\n",
    "scores = []\n",
    "\n",
    "for episode in range(nr_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not(done):\n",
    "        #***YOUR CODE HERE***\n",
    "        # sample a epsilon-greedy action from the action space\n",
    "        # take the action and get the new observation space\n",
    "        # Q-learning \n",
    "        obs = new_obs \n",
    "        \n",
    "        if done:\n",
    "            scores_window.append(reward)       # save last reward received\n",
    "            if debug_:\n",
    "              print(reward)\n",
    "            #print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)), end=\"\")\n",
    "            if episode % 1000 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "                scores.append( np.mean(scores_window) )  \n",
    "    \n",
    "#np.savez('Q_Q-Learning.npz',Q=Q)   # uncomment to save your result\n",
    "#data = np.load('Q_Q-Learning.npz') # uncomment to load data\n",
    "#Q = data['Q']                      # uncomment to load data\n",
    "\n",
    "plot_training(np.arange(0,nr_episodes,1000),scores,fout='Q_Q-Learning.jpg')    \n",
    "    \n",
    "ani = animation_result(Q)\n",
    "\n",
    "#%% \n",
    "#env.close()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c37a01",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b827b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3972f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
